


# Filtering Warnings
import warnings
warnings.filterwarnings('ignore')

#Import Libraries
import numpy as np
import pandas as pd
#import seaborn as sns
from matplotlib import pyplot as plt
from plotly.subplots import make_subplots
import plotly.graph_objects as go
pd.set_option('display.max_columns', 300) #Setting column display limit
plt.style.use('ggplot') #Applying style to graphs






app_df = pd.read_parquet('data/application_data.parquet')






app_df.shape



app_df.head()


app_df.describe()





#null checking

(app_df.isnull().sum()/len(app_df)*100).sort_values(ascending = False).head(50)





null_col = app_df.isnull().sum().sort_values(ascending = False)
null_col = null_col[null_col.values >(0.40*len(app_df))]


#Plotting Bar Graph for null values greater than 40%

plt.figure(figsize=(20,4))
null_col.plot(kind='bar', color="#4CB391")
plt.title('List of Columns & null counts where null values are more than 40%')

plt.xlabel("Null Columns",fontdict={"fontsize":12,"fontweight":5})
plt.ylabel("Count of null values",fontdict={"fontsize":12,"fontweight":5})
plt.show()


len(null_col)





#Correlated columns with target

correlation=app_df.corr(numeric_only=True)
top_positive_corr = correlation.TARGET.sort_values(ascending=False).head(5).index.to_list()
top_negative_corr = correlation.TARGET.sort_values(ascending=True).head(5).index.to_list()
print(top_positive_corr)
print(top_negative_corr)


top_corr_columns = []
top_corr_columns = top_positive_corr + top_negative_corr
print(top_corr_columns)


cols_to_drop = list(null_col.index.values) #Making list of column names having null values greater than 40%


#Removing top correlated columns from columns to be dropped

for col in top_corr_columns:
    if col in cols_to_drop:
        cols_to_drop.remove(col)
        print(col)


len(cols_to_drop)


app_df.drop(labels = cols_to_drop,axis=1,inplace = True) #Dropping those columns


app_df = app_df[top_corr_columns]


app_df.shape


#Checking for left-out columns with null

null_left = (app_df.isnull().sum()/len(app_df)*100).sort_values(ascending = False).head(20)
null_left





app_df.EXT_SOURCE_3.fillna(app_df.EXT_SOURCE_3.median() , inplace = True)

app_df.EXT_SOURCE_2.fillna(app_df.EXT_SOURCE_2.median() , inplace = True)

app_df.EXT_SOURCE_1.fillna(app_df.EXT_SOURCE_1.median() , inplace = True)


#Since this is an average computation based column, hence we fill NaN values with median

app_df['FLOORSMAX_AVG'].fillna(app_df['FLOORSMAX_AVG'].median(),inplace=True)


#DAYS_LAST_PHONE_CHANGE

app_df['DAYS_LAST_PHONE_CHANGE'].head(5)


'''
This is a numeric column,
1. values will be standardized by taking absolute values
2. replace few missing values with Mode
'''
app_df['DAYS_LAST_PHONE_CHANGE']=app_df['DAYS_LAST_PHONE_CHANGE'].abs()


app_df['DAYS_LAST_PHONE_CHANGE'].fillna(app_df['DAYS_LAST_PHONE_CHANGE'].mode,inplace=True)


null_check = (app_df.isnull().sum()/len(app_df)*100).sort_values(ascending = False)
null_check








app_df.head()


app_df['DAYS_BIRTH'] = app_df['DAYS_BIRTH'].abs()
app_df['DAYS_EMPLOYED'] = app_df['DAYS_EMPLOYED'].abs()


app_df.head()


#DAYS_BIRTH

app_df['DAYS_BIRTH']= (app_df['DAYS_BIRTH']/365).astype(int)
app_df['DAYS_BIRTH'].unique()


app_df['AGE_GROUP']=pd.cut(app_df['DAYS_BIRTH'],
                         bins=[19,25,40,60,100], labels=['Very_Young','Youth', 'Middle_Age', 'Elder'])


app_df[['DAYS_BIRTH','AGE_GROUP']]


app_df['DAYS_EMPLOYED']= (app_df['DAYS_EMPLOYED']/365).astype(int)
app_df['DAYS_EMPLOYED'].unique()


#Bin into groups
app_df['DAYS_EMPLOYED']=pd.cut(app_df['DAYS_EMPLOYED'],
                         bins=[19,25,40,60,100], labels=['Very_Young','Youth', 'Middle_Age', 'Elder'])


# Create a new column 'AMT_CREDIT_GROUP' based on the bins and labels

app_df['AMT_CREDIT_GROUP'] = pd.cut(app_df['AMT_CREDIT'], bins=bin_edges, labels=bin_labels, right=False)


app_df['AMT_CREDIT_GROUP'].value_counts()


#AMT_INCOME_TOTAL

app_df.AMT_INCOME_TOTAL.describe()


income_bins = [25650, 75000, 112500, 135000, 157500, 180000, 202500, 225000, 117000000]
income_labels = ["26k to 75k", "75k to 112.5k", "112.5k to 135k", "135k to 157.5k", "157.5k to 180k", "180k to 202.5k", "202.5k to 225k", "225k to 117M"]


# Creating a new column INCOME_GROUP
app_df['AMT_INCOME_GROUP'] = pd.cut(app_df['AMT_INCOME_TOTAL'], bins=income_bins, labels=income_labels)


app_df.AMT_INCOME_GROUP.value_counts()





app_df.shape


app_df.head()


app_df.isnull().sum().sort_values(ascending=False).head()





amt_credit_grp_null = app_df[app_df['AMT_CREDIT_GROUP'].isnull()]
amt_credit_grp_null[['AMT_CREDIT','AMT_CREDIT_GROUP']].head(8)


amt_inc_grp_null = app_df[app_df['AMT_INCOME_GROUP'].isnull()]
amt_inc_grp_null[['AMT_INCOME_TOTAL','AMT_INCOME_GROUP']].head(2)





app_df.AMT_CREDIT_GROUP.fillna('2.5M-4.05M',inplace = True)
app_df.AMT_INCOME_GROUP.fillna('225k to 117M',inplace = True)


app_df.isnull().sum().sort_values(ascending=False).head()








#DATA IMBALANCE CHECK

target_counts = app_df['TARGET'].value_counts()
target_counts


# Create a pie chart
plt.figure(figsize=(6, 6))
plt.pie(target_counts, labels=target_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of TARGET Variable')
plt.show()





Target0 = app_df.loc[app_df["TARGET"]==0]
Target1 = app_df.loc[app_df["TARGET"]==1]

round(len(Target0)/len(Target1),2)








#The datatype of categorical columns below will be changed to category to suit univariate analysis
app_df['NAME_CONTRACT_TYPE'] = app_df['NAME_CONTRACT_TYPE'].astype('category')
app_df['CODE_GENDER'] = app_df['CODE_GENDER'].astype('category')
app_df['NAME_TYPE_SUITE'] = app_df['NAME_TYPE_SUITE'].astype('category')
app_df['NAME_INCOME_TYPE'] = app_df['NAME_INCOME_TYPE'].astype('category')
app_df['NAME_EDUCATION_TYPE'] = app_df['NAME_EDUCATION_TYPE'].astype('category')
app_df['NAME_FAMILY_STATUS'] = app_df['NAME_FAMILY_STATUS'].astype('category')
app_df['NAME_HOUSING_TYPE'] = app_df['NAME_HOUSING_TYPE'].astype('category')
app_df['OCCUPATION_TYPE'] = app_df['OCCUPATION_TYPE'].astype('category')
app_df['WEEKDAY_APPR_PROCESS_START'] = app_df['WEEKDAY_APPR_PROCESS_START'].astype('category')
app_df['ORGANIZATION_TYPE'] = app_df['ORGANIZATION_TYPE'].astype('category')


#CONVERTING OBJECT DATATYPES TO NUMERICAL OR CATEGORICAL DATATYPE

app_df.select_dtypes(include='object').info()

app_df['FLAG_OWN_CAR'] = app_df['FLAG_OWN_CAR'].astype('category')
app_df['FLAG_OWN_REALTY'] = app_df['FLAG_OWN_REALTY'].astype('category')
app_df['DAYS_LAST_PHONE_CHANGE'] = pd.to_numeric(app_df['DAYS_LAST_PHONE_CHANGE'], errors= 'coerce')


#FINAL NULL CHECKING AND HANDLING

app_df.isnull().sum().sort_values(ascending=False).head()


days_last_phone_change_null = app_df[app_df['DAYS_LAST_PHONE_CHANGE'].isnull()]
days_last_phone_change_null


app_df['DAYS_LAST_PHONE_CHANGE'].fillna(app_df['DAYS_LAST_PHONE_CHANGE'].median(),inplace=True)
app_df.isnull().sum().sort_values(ascending=False).head()


'''
- After observations, these columns are recommended to be dropped due to less relevance for analysis and modelling
- A column is to be dropped provided it is not highly correlated with the target column
'''
drop_this_columns =['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE',
       'FLAG_PHONE', 'FLAG_EMAIL','FLAG_EMAIL',
       'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3','FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6',
       'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9','FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12',
       'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15','FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18',
       'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21']


#confirming that top correlated columns are not included among columns recommended to be dropped

for col in top_corr_columns:
    if col in drop_this_columns:
        drop_this_columns.remove(col)
        print(col)


app_df[drop_this_columns].head()


app_df.drop(labels=drop_this_columns,axis=1,inplace=True)


app_df.shape





from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
from sklearn.metrics import roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
import pickle
import gc


import lightgbm as lgb


model_df = app_df.drop(['SK_ID_CURR'],axis=1)


model_df.head()


y_label = model_df.pop('TARGET').values


y_label


model_df.head()


X_train, X_temp, y_train, y_temp = train_test_split(model_df, y_label, stratify = y_label, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, stratify = y_temp, test_size=0.5, random_state=42)
print('Shape of X_train:',X_train.shape)
print('Shape of X_val:',X_val.shape)
print('Shape of X_test:',X_test.shape)





# Seperation of columns into numeric and categorical columns
num_cols = np.array(X_train.select_dtypes(include= ['int64','float64']).columns)
cat_cols = np.array(X_train.select_dtypes(include= ['category','object']).columns)


#NUMERIC COLUMN STANDARDIZATION
X_train_num = X_train[num_cols]
X_val_num = X_val[num_cols]
X_test_num = X_test[num_cols]


#SCALING
scaler_num = StandardScaler()
X_train_num_scaled = scaler_num.fit_transform(X_train_num)
X_val_num_scaled = scaler_num.transform(X_val_num)
X_test_num_scaled = scaler_num.transform(X_test_num)

X_train_num_final = pd.DataFrame(X_train_num_scaled, columns=num_cols)
X_val_num_final = pd.DataFrame(X_val_num_scaled, columns=num_cols)
X_test_num_final = pd.DataFrame(X_test_num_scaled, columns=num_cols)


#CATEGORICAL COLUMN STANDARDIZATION

X_train_cat = X_train[cat_cols]
X_val_cat = X_val[cat_cols]
X_test_cat = X_test[cat_cols]

X_train_cat1= pd.DataFrame(X_train_cat, columns=cat_cols)
X_val_cat1= pd.DataFrame(X_val_cat, columns=cat_cols)
X_test_cat1= pd.DataFrame(X_test_cat, columns=cat_cols)

#ONE HOT ENCODING
ohe = OneHotEncoder(sparse=False,handle_unknown='ignore')
X_train_cat2 = ohe.fit_transform(X_train_cat1)
X_val_cat2 = ohe.transform(X_val_cat1)
X_test_cat2 = ohe.transform(X_test_cat1)

cat_cols_ohe = list(ohe.get_feature_names_out(input_features=cat_cols))
X_train_cat_final = pd.DataFrame(X_train_cat2, columns = cat_cols_ohe)
X_val_cat_final = pd.DataFrame(X_val_cat2, columns = cat_cols_ohe)
X_test_cat_final = pd.DataFrame(X_test_cat2, columns = cat_cols_ohe)


# STANDARDIZED DATA

X_train_final = pd.concat([X_train_num_final,X_train_cat_final], axis = 1)
X_val_final = pd.concat([X_val_num_final,X_val_cat_final], axis = 1)
X_test_final = pd.concat([X_test_num_final,X_test_cat_final], axis = 1)
print(X_train_final.shape)
print(X_val_final.shape)
print(X_test_final.shape)


#RENAMING SPECIAL CHARACTER COLUMN NAMES

import re
X_train_final = X_train_final.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))
X_val_final = X_val_final.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))
X_test_final = X_test_final.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))





# Saving the Dataframes into CSV files
X_train_final.to_csv('X_train_final.csv')
X_val_final.to_csv('X_val_final.csv')
X_test_final.to_csv('X_test_final.csv')
# Saving the numpy arrays into text files
np.savetxt('y_label.txt', y_label)
np.savetxt('y_train.txt', y_train)
np.savetxt('y_val.txt', y_val)
np.savetxt('y_test.txt', y_test)





model_sk = lgb.LGBMClassifier(boosting_type='gbdt', max_depth=7, learning_rate=0.01, n_estimators= 2000,
                 class_weight='balanced', subsample=0.9, colsample_bytree= 0.8, n_jobs=-1, early_stopping_rounds=100)

train_features, val_features, train_y, val_y = train_test_split(X_train_final, y_train, test_size = 0.15, random_state = 42)

model_sk.fit(train_features, train_y, eval_set = [(val_features, val_y)], eval_metric = 'auc')


#FEATURE IMPORTANCE

feature_imp = pd.DataFrame(sorted(zip(model_sk.feature_importances_, X_train_final.columns)), columns=['Value','Feature'])
features_df = feature_imp.sort_values(by="Value", ascending=False)
selected_features = list(features_df[features_df['Value']>=50]['Feature'])

# Save selected features into pickle file
with open('select_features.txt','wb') as pf:
    pickle.dump(selected_features, pf)
print('The no. of features selected:',len(selected_features))


# Feature importance Plot

top50_features = features_df.head(50)

# Create a data frame for visualization.
top50_features_df = pd.DataFrame({"Features": pd.DataFrame(top50_features).sort_values(by='Value')['Feature'],
                                  "Importances": top50_features.sort_values(by='Value')['Value']})

# Plot the feature importances in bars.
plt.figure(figsize=(19, 6))
plt.bar(top50_features_df['Features'], top50_features_df['Importances'], color='teal')
plt.xticks(rotation=90)
plt.title("Top 50 Important Features")
plt.show()





def plot_confusion_matrix(test_y, predicted_y):
    # Confusion matrix
    C = confusion_matrix(test_y, predicted_y)

    # Recall matrix
    A = (((C.T)/(C.sum(axis=1))).T)

    # Precision matrix
    B = (C/C.sum(axis=0))

    plt.figure(figsize=(20,4))

    labels = ['Re-paid(0)','Not Re-paid(1)']
    cmap=sns.light_palette("purple")
    plt.subplot(1,3,1)
    sns.heatmap(C, annot=True, cmap=cmap,fmt="d", xticklabels = labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title('Confusion matrix')

    plt.subplot(1,3,2)
    sns.heatmap(A, annot=True, cmap=cmap, xticklabels = labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title('Recall matrix')

    plt.subplot(1,3,3)
    sns.heatmap(B, annot=True, cmap=cmap, xticklabels = labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title('Precision matrix')

    plt.show()


def cv_plot(alpha, cv_auc):

    fig, ax = plt.subplots()
    ax.plot(np.log10(alpha), cv_auc,c='g')
    for i, txt in enumerate(np.round(cv_auc,3)):
        ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_auc[i]))
    plt.grid()
    plt.xticks(np.log10(alpha))
    plt.title("Cross Validation Error for each alpha")
    plt.xlabel("Alpha i's")
    plt.ylabel("Error measure")
    plt.show()





model_results = [[],[],[]]


#LOGISTIC REGRESSION

alpha = np.logspace(-4,4,9)
cv_auc_score = []
for i in alpha:
    clf = SGDClassifier(alpha=i, penalty='l1',class_weight = 'balanced', loss='log_loss', random_state=28)
    clf.fit(X_train_final[selected_features], y_train)
    sig_clf = CalibratedClassifierCV(clf, method='sigmoid')
    sig_clf.fit(X_train_final[selected_features], y_train)
    y_pred_prob = sig_clf.predict_proba(X_val_final[selected_features])[:,1]
    cv_auc_score.append(roc_auc_score(y_val,y_pred_prob))
    print('For alpha {0}, cross validation AUC score {1}'.format(i,roc_auc_score(y_val,y_pred_prob)))
cv_plot(alpha, cv_auc_score)
print('The Optimal C value is:', alpha[np.argmax(cv_auc_score)])


best_alpha = alpha[np.argmax(cv_auc_score)]
logreg = SGDClassifier(alpha = best_alpha, class_weight = 'balanced', penalty = 'l1', loss='log_loss', random_state = 28)
logreg.fit(X_train_final[selected_features], y_train)
logreg_sig_clf = CalibratedClassifierCV(logreg, method='sigmoid')
logreg_sig_clf.fit(X_train_final[selected_features], y_train)
y_pred_prob = logreg_sig_clf.predict_proba(X_train_final[selected_features])[:,1]
print('For best alpha {0}, The Train AUC score is {1}'.format(best_alpha, roc_auc_score(y_train,y_pred_prob)))
model_results[0].append(roc_auc_score(y_train,y_pred_prob))
y_pred_prob = logreg_sig_clf.predict_proba(X_val_final[selected_features])[:,1]
print('For best alpha {0}, The Cross validated AUC score is {1}'.format(best_alpha, roc_auc_score(y_val,y_pred_prob)))
model_results[0].append(roc_auc_score(y_val,y_pred_prob))
y_pred_prob = logreg_sig_clf.predict_proba(X_test_final[selected_features])[:,1]
print('For best alpha {0}, The Test AUC score is {1}'.format(best_alpha, roc_auc_score(y_test,y_pred_prob)))
model_results[0].append(roc_auc_score(y_test,y_pred_prob))
y_pred = logreg.predict(X_test_final[selected_features])
print('The test AUC score is :', roc_auc_score(y_test,y_pred_prob))
print('The test accuracy score is :', accuracy_score(y_test, y_pred))
model_results[0].append(accuracy_score(y_test,y_pred))
print('The percentage of misclassified points {:05.2f}% :'.format((1-accuracy_score(y_test, y_pred))*100))
plot_confusion_matrix(y_test, y_pred)


from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
auc = roc_auc_score(y_test,y_pred_prob)
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, marker='.')
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title('ROC curve', fontsize = 20)
plt.xlabel('FPR', fontsize=15)
plt.ylabel('TPR', fontsize=15)
plt.grid()
plt.legend(["AUC=get_ipython().run_line_magic(".3f"%auc])", "")
plt.show()


#RANDOM FOREST CLASSIFIER

alpha = [200,500,1000]
max_depth = [7, 10]
cv_auc_score = []
for i in alpha:
    for j in max_depth:
        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j,class_weight='balanced',
                                     random_state=42, n_jobs=-1)
        clf.fit(X_train_final[selected_features], y_train)
        sig_clf = CalibratedClassifierCV(clf, method="sigmoid")
        sig_clf.fit(X_train_final[selected_features], y_train)
        y_pred_prob = sig_clf.predict_proba(X_val_final[selected_features])[:,1]
        cv_auc_score.append(roc_auc_score(y_val,y_pred_prob))
        print('For n_estimators {0}, max_depth {1} cross validation AUC score {2}'.
              format(i,j,roc_auc_score(y_val,y_pred_prob)))


best_alpha = np.argmax(cv_auc_score)
print('The optimal values are: n_estimators {0}, max_depth {1} '.format(alpha[int(best_alpha/2)],
                                                                        max_depth[int(best_alpha%2)]))
rf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)],
                            class_weight='balanced', random_state=42, n_jobs=-1)
rf.fit(X_train_final[selected_features], y_train)
rf_sig_clf = CalibratedClassifierCV(rf, method="sigmoid")
rf_sig_clf.fit(X_train_final[selected_features], y_train)
y_pred_prob = rf_sig_clf.predict_proba(X_train_final[selected_features])[:,1]
print('For best n_estimators {0} best max_depth {1}, The Train AUC score is {2}'.format(alpha[int(best_alpha/2)],
                                                    max_depth[int(best_alpha%2)],roc_auc_score(y_train,y_pred_prob)))
model_results[1].append(roc_auc_score(y_train,y_pred_prob))
y_pred_prob = rf_sig_clf.predict_proba(X_val_final[selected_features])[:,1]
print('For best n_estimators {0} best max_depth {1}, The Validation AUC score is {2}'.format(alpha[int(best_alpha/2)],
                                                            max_depth[int(best_alpha%2)],roc_auc_score(y_val,y_pred_prob)))
model_results[1].append(roc_auc_score(y_val,y_pred_prob))
y_pred_prob = rf_sig_clf.predict_proba(X_test_final[selected_features])[:,1]
print('For best n_estimators {0} best max_depth {1}, The Test AUC score is {2}'.format(alpha[int(best_alpha/2)],
                                                        max_depth[int(best_alpha%2)],roc_auc_score(y_test,y_pred_prob)))
model_results[1].append(roc_auc_score(y_test,y_pred_prob))
y_pred = rf_sig_clf.predict(X_test_final[selected_features])
print('The test AUC score is :', roc_auc_score(y_test,y_pred_prob))
print('The accuracy score is :', accuracy_score(y_test, y_pred))
model_results[1].append(accuracy_score(y_test,y_pred))
print('The percentage of misclassified points {:05.2f}% :'.format((1-accuracy_score(y_test, y_pred))*100))
plot_confusion_matrix(y_test, y_pred)


from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
auc = roc_auc_score(y_test,y_pred_prob)
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, marker='.')
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title('ROC curve', fontsize = 20)
plt.xlabel('FPR', fontsize=15)
plt.ylabel('TPR', fontsize=15)
plt.grid()
plt.legend(["AUC=get_ipython().run_line_magic(".3f"%auc])", "")
plt.show()


#LIGHTGBM

weight = np.ones((len(X_train_final),), dtype=int)
for i in range(len(X_train_final)):
    if y_train[i]== 0:
        weight[i]=1
    else:
        weight[i]=11

train_data=lgb.Dataset(X_train_final[selected_features], label = y_train, weight= weight )
valid_data=lgb.Dataset(X_val_final[selected_features], label = y_val)
cv_auc_score = []
max_depth = [3, 5, 7, 10]
for i in max_depth:

    params = {'boosting_type': 'gbdt',
          'max_depth' : i,
          'objective': 'binary',
          'nthread': 5,
          'num_leaves': 32,
          'learning_rate': 0.05,
          'max_bin': 512,
          'subsample_for_bin': 200,
          'subsample': 0.7,
          'subsample_freq': 1,
          'colsample_bytree': 0.8,
          'reg_alpha': 20,
          'reg_lambda': 20,
          'min_split_gain': 0.5,
          'min_child_weight': 1,
          'min_child_samples': 10,
          'scale_pos_weight': 1,
          'num_class' : 1,
          'metric' : 'auc'
          }
lgbm = lgb.train(params,
              train_data,
              2500,
              valid_sets=valid_data
              )
y_pred_prob = lgbm.predict(X_val_final[selected_features])
cv_auc_score.append(roc_auc_score(y_val,y_pred_prob))
print('For  max_depth {0} and some other parameters, cross validation AUC score {1}'.format(i,roc_auc_score(y_val,y_pred_prob)))
print('The optimal  max_depth: ', max_depth[np.argmax(cv_auc_score)])
params = {'boosting_type': 'gbdt',
      'max_depth' : max_depth[np.argmax(cv_auc_score)],
      'objective': 'binary',
      'nthread': 5,
      'num_leaves': 32,
      'learning_rate': 0.05,
      'max_bin': 512,
      'subsample_for_bin': 200,
      'subsample': 0.7,
      'subsample_freq': 1,
      'colsample_bytree': 0.8,
      'reg_alpha': 20,
      'reg_lambda': 20,
      'min_split_gain': 0.5,
      'min_child_weight': 1,
      'min_child_samples': 10,
      'scale_pos_weight': 1,
      'num_class' : 1,
      'metric' : 'auc'
      }
lgbm = lgb.train(params,
                 train_data,
                 2500,
                 valid_sets=valid_data
                 )
y_pred_prob = lgbm.predict(X_train_final[selected_features])
print('For best max_depth {0}, The Train AUC score is {1}'.format(max_depth[np.argmax(cv_auc_score)],
                                                                  roc_auc_score(y_train,y_pred_prob) ))
model_results[2].append(roc_auc_score(y_train,y_pred_prob))
y_pred_prob = lgbm.predict(X_val_final[selected_features])
print('For best max_depth {0}, The Cross validated AUC score is {1}'.format(max_depth[np.argmax(cv_auc_score)],
                                                                            roc_auc_score(y_val,y_pred_prob) ))
model_results[2].append(roc_auc_score(y_val,y_pred_prob))
y_pred_prob = lgbm.predict(X_test_final[selected_features])
print('For best max_depth {0}, The Test AUC score is {1}'.format(max_depth[np.argmax(cv_auc_score)],
                                                                 roc_auc_score(y_test,y_pred_prob) ))
model_results[2].append(roc_auc_score(y_test,y_pred_prob))
y_pred = np.ones((len(X_test_final),), dtype=int)
for i in range(len(y_pred_prob)):
    if y_pred_prob[i]<=0.5:
        y_pred[i]=0
    else:
        y_pred[i]=1
print('The test AUC score is :', roc_auc_score(y_test,y_pred_prob))
print('The accuracy score is :', accuracy_score(y_test, y_pred))
model_results[2].append(accuracy_score(y_test,y_pred))
print('The percentage of misclassified points {:05.2f}% :'.format((1-accuracy_score(y_test, y_pred))*100))
plot_confusion_matrix(y_test, y_pred)


from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
auc = roc_auc_score(y_test,y_pred_prob)
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, marker='.')
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title('ROC curve', fontsize = 20)
plt.xlabel('FPR', fontsize=15)
plt.ylabel('TPR', fontsize=15)
plt.grid()
plt.legend(["AUC=get_ipython().run_line_magic(".3f"%auc])", "")
plt.show()


eval_score = pd.DataFrame(model_results, columns=['Train AUC','Cross Val AUC','Test AUC','Accuracy'], index=['Logistic Regression','Random Forest','LightGBM'])
eval_score


# Saving the best performing LightGBM model as a pickle file for the future use
with open('final_model.pkl','wb') as fp:
    pickle.dump(lgbm, fp)
